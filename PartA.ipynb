{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------\n",
        "# Create virtual display (Google Colab)\n",
        "# --------------------------------------------\n",
        "!pip install svgling\n",
        "!apt-get install -y xvfb ghostscript python3-tk # Install X Virtual Frame Buffer and other dependencies\n",
        "import os\n",
        "os.system('Xvfb :1 -screen 0 1600x1200x16  &')    # create virtual display with size 1600x1200 and 16 bit color. Color can be changed to 24 or 8\n",
        "os.environ['DISPLAY']=':1.0'    # tell X clients to use our virtual DISPLAY :1.0.\n",
        "%matplotlib inline\n"
      ],
      "metadata": {
        "id": "ZLZaxJuQEysx",
        "outputId": "02535ca7-f415-4e1f-8aa7-0020323d31ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ZLZaxJuQEysx",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: svgling in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: svgwrite in /usr/local/lib/python3.11/dist-packages (from svgling) (1.4.3)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ghostscript is already the newest version (9.55.0~dfsg1-0ubuntu5.12).\n",
            "python3-tk is already the newest version (3.10.8-1~22.04).\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.15).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a5041354",
      "metadata": {
        "id": "a5041354"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import json\n",
        "import pprint\n",
        "from pprint import pprint\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "import string\n",
        "from nltk.tree import Tree\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "from nltk import pos_tag, FreqDist, ConditionalFreqDist, CFG\n",
        "from nltk.util import bigrams\n",
        "from nltk.tag import RegexpTagger\n",
        "from nltk.probability import ConditionalProbDist, MLEProbDist, LidstoneProbDist\n",
        "from nltk.parse.chart import ChartParser\n",
        "from nltk.lm import MLE, Laplace\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.util import ngrams\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6b129150",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b129150",
        "outputId": "c1900f1e-166f-4821-a3d4-fca0483f8a55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "# Downloads\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# Function for cleaner display format - Miyoko\n",
        "def print_in_chunks(token_list, chunk_size=5):\n",
        "    for i in range(0, len(token_list), chunk_size):\n",
        "        print(token_list[i:i+chunk_size])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Import Repo from github\n",
        "# -----------------------------\n",
        "!git clone https://github.com/MiyokoPang/TXSA.git"
      ],
      "metadata": {
        "id": "HzpzGmHgEE5W"
      },
      "id": "HzpzGmHgEE5W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e871458b",
      "metadata": {
        "id": "e871458b"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Data_1.txt Import\n",
        "# -----------------------------\n",
        "with open(\"TXSA/Data_1.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    data_1 = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2ca62a84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ca62a84",
        "outputId": "1d531863-33f4-452e-9d31-7f90ee54d59e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1 split function\n",
            "['Classification', 'is', 'the', 'task', 'of']\n",
            "['choosing', 'the', 'correct', 'class', 'label']\n",
            "['for', 'a', 'given', 'input.', 'In']\n",
            "['basic', 'classification', 'tasks,', 'each', 'input']\n",
            "['is', 'considered', 'in', 'isolation', 'from']\n",
            "['all', 'other', 'inputs,', 'and', 'the']\n",
            "['set', 'of', 'labels', 'is', 'defined']\n",
            "['in', 'advance.', 'The', 'basic', 'classification']\n",
            "['task', 'has', 'a', 'number', 'of']\n",
            "['interesting', 'variants.', 'For', 'example,', 'in']\n",
            "['multiclass', 'classification,', 'each', 'instance', 'may']\n",
            "['be', 'assigned', 'multiple', 'labels;', 'in']\n",
            "['open-class', 'classification,', 'the', 'set', 'of']\n",
            "['labels', 'is', 'not', 'defined', 'in']\n",
            "['advance;', 'and', 'in', 'sequence', 'classification,']\n",
            "['a', 'list', 'of', 'inputs', 'are']\n",
            "['jointly', 'classified.']\n",
            "Time taken: 107.53 ms\n"
          ]
        }
      ],
      "source": [
        "# Q1 Word Tokenization\n",
        "# split() function - Miyoko Pang\n",
        "print('Q1 split function')\n",
        "start = time.time()\n",
        "tokenization_1 = data_1.split()\n",
        "end = time.time()\n",
        "print_in_chunks(tokenization_1, 5)\n",
        "print(f\"Time taken: {(end - start) * 1000000:.2f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f8f0c6b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8f0c6b7",
        "outputId": "3f91f766-be49-460a-c556-dd88a7ae094e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1 RE function\n",
            "['Classification', 'is', 'the', 'task', 'of']\n",
            "['choosing', 'the', 'correct', 'class', 'label']\n",
            "['for', 'a', 'given', 'input', 'In']\n",
            "['basic', 'classification', 'tasks', 'each', 'input']\n",
            "['is', 'considered', 'in', 'isolation', 'from']\n",
            "['all', 'other', 'inputs', 'and', 'the']\n",
            "['set', 'of', 'labels', 'is', 'defined']\n",
            "['in', 'advance', 'The', 'basic', 'classification']\n",
            "['task', 'has', 'a', 'number', 'of']\n",
            "['interesting', 'variants', 'For', 'example', 'in']\n",
            "['multiclass', 'classification', 'each', 'instance', 'may']\n",
            "['be', 'assigned', 'multiple', 'labels', 'in']\n",
            "['open', 'class', 'classification', 'the', 'set']\n",
            "['of', 'labels', 'is', 'not', 'defined']\n",
            "['in', 'advance', 'and', 'in', 'sequence']\n",
            "['classification', 'a', 'list', 'of', 'inputs']\n",
            "['are', 'jointly', 'classified']\n",
            "\n",
            "Total number of tokens: 83\n",
            "Time taken: 217.91 µs\n"
          ]
        }
      ],
      "source": [
        "# Regular Expression function - Yi Jing\n",
        "print('Q1 RE function')\n",
        "start_time = time.time()\n",
        "tokens = re.findall(r'\\b\\w+\\b', data_1)\n",
        "end_time = time.time()\n",
        "\n",
        "print_in_chunks(tokens)\n",
        "print(f\"\\nTotal number of tokens: {len(tokens)}\")\n",
        "print(f\"Time taken: {(end_time - start_time) * 1000000:.2f} µs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "95fd3ff7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95fd3ff7",
        "outputId": "5d940bcf-3417-4e67-9fe1-7154ead85bca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1 NLTK function\n",
            "['Classification', 'is', 'the', 'task', 'of']\n",
            "['choosing', 'the', 'correct', 'class', 'label']\n",
            "['for', 'a', 'given', 'input', '.']\n",
            "['In', 'basic', 'classification', 'tasks', ',']\n",
            "['each', 'input', 'is', 'considered', 'in']\n",
            "['isolation', 'from', 'all', 'other', 'inputs']\n",
            "[',', 'and', 'the', 'set', 'of']\n",
            "['labels', 'is', 'defined', 'in', 'advance']\n",
            "['.', 'The', 'basic', 'classification', 'task']\n",
            "['has', 'a', 'number', 'of', 'interesting']\n",
            "['variants', '.', 'For', 'example', ',']\n",
            "['in', 'multiclass', 'classification', ',', 'each']\n",
            "['instance', 'may', 'be', 'assigned', 'multiple']\n",
            "['labels', ';', 'in', 'open-class', 'classification']\n",
            "[',', 'the', 'set', 'of', 'labels']\n",
            "['is', 'not', 'defined', 'in', 'advance']\n",
            "[';', 'and', 'in', 'sequence', 'classification']\n",
            "[',', 'a', 'list', 'of', 'inputs']\n",
            "['are', 'jointly', 'classified', '.']\n",
            "Time taken: 40801.76 ms\n"
          ]
        }
      ],
      "source": [
        "# NLTK function - Shu Hui\n",
        "print('Q1 NLTK function')\n",
        "start = time.time()\n",
        "tokenization = word_tokenize(data_1)\n",
        "end = time.time()\n",
        "print_in_chunks(tokenization, 5)\n",
        "print(f\"Time taken: {(end - start) * 1000000:.2f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d0d1c7ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0d1c7ef",
        "outputId": "17c3476f-055f-44cb-cfbb-4de34d415560"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1.3 Stop Words & Punctuation Removal\n",
            "Original Token Count: 94\n",
            "Filtered Token Count: 45\n",
            "\n",
            "Filtered Tokens:\n",
            "['Classification', 'task', 'choosing', 'correct', 'class']\n",
            "['label', 'given', 'input', 'basic', 'classification']\n",
            "['tasks', 'input', 'considered', 'isolation', 'inputs']\n",
            "['set', 'labels', 'defined', 'advance', 'basic']\n",
            "['classification', 'task', 'number', 'interesting', 'variants']\n",
            "['example', 'multiclass', 'classification', 'instance', 'may']\n",
            "['assigned', 'multiple', 'labels', 'open-class', 'classification']\n",
            "['set', 'labels', 'defined', 'advance', 'sequence']\n",
            "['classification', 'list', 'inputs', 'jointly', 'classified']\n",
            "\n",
            "Stop Words Found and Removed:\n",
            "['is', 'the', 'of', 'the', 'for']\n",
            "['a', 'in', 'each', 'is', 'in']\n",
            "['from', 'all', 'other', 'and', 'the']\n",
            "['of', 'is', 'in', 'the', 'has']\n",
            "['a', 'of', 'for', 'in', 'each']\n",
            "['be', 'in', 'the', 'of', 'is']\n",
            "['not', 'in', 'and', 'in', 'a']\n",
            "['of', 'are']\n",
            "\n",
            "Total Stop Words Removed: 37\n",
            "Time taken: 6453.28 µs\n",
            "-----------------------------\n"
          ]
        }
      ],
      "source": [
        "# Q1.3: Stop Words & Punctuation Removal - Yi Jing\n",
        "print(\"Q1.3 Stop Words & Punctuation Removal\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Tokenize\n",
        "tokens = word_tokenize(data_1)\n",
        "\n",
        "# Prepare stop words and punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "# Remove stop words and punctuation\n",
        "filtered_tokens = []\n",
        "found_stopwords = []\n",
        "\n",
        "for word in tokens:\n",
        "    word_lower = word.lower()\n",
        "    if word_lower in stop_words:\n",
        "        found_stopwords.append(word_lower)\n",
        "    elif word not in punctuation:\n",
        "        filtered_tokens.append(word)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Print results\n",
        "print(\"Original Token Count:\", len(tokens))\n",
        "print(\"Filtered Token Count:\", len(filtered_tokens))\n",
        "\n",
        "print(\"\\nFiltered Tokens:\")\n",
        "print_in_chunks(filtered_tokens)\n",
        "\n",
        "print(\"\\nStop Words Found and Removed:\")\n",
        "print_in_chunks(found_stopwords)\n",
        "\n",
        "print(f\"\\nTotal Stop Words Removed: {len(found_stopwords)}\")\n",
        "print(f\"Time taken: {(end_time - start_time) * 1000000:.2f} µs\")\n",
        "\n",
        "print('-----------------------------')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "56a0a1a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56a0a1a7",
        "outputId": "27c30ef7-a63b-4a91-de45-dbbacef29beb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q2 RE function\n",
            "['classification', 'i', 'the', 'task', 'of']\n",
            "['choos', 'the', 'correct', 'clas', 'label']\n",
            "['for', 'a', 'given', 'input', 'in']\n",
            "['basic', 'classification', 'task', 'each', 'input']\n",
            "['i', 'consider', 'in', 'isolation', 'from']\n",
            "['all', 'other', 'input', 'and', 'the']\n",
            "['set', 'of', 'label', 'i', 'defin']\n",
            "['in', 'advance', 'the', 'basic', 'classification']\n",
            "['task', 'ha', 'a', 'number', 'of']\n",
            "['interest', 'variant', 'for', 'example', 'in']\n",
            "['multiclas', 'classification', 'each', 'instance', 'may']\n",
            "['be', 'assign', 'multiple', 'label', 'in']\n",
            "['open', 'clas', 'classification', 'the', 'set']\n",
            "['of', 'label', 'i', 'not', 'defin']\n",
            "['in', 'advance', 'and', 'in', 'sequence']\n",
            "['classification', 'a', 'list', 'of', 'input']\n",
            "['are', 'joint', 'classifi']\n",
            "Time taken: 504.26 µs\n"
          ]
        }
      ],
      "source": [
        "# Q2 Form Word Stemming\n",
        "# Regular Expression function - Yi Jing\n",
        "print('Q2 RE function')\n",
        "start_time = time.time()\n",
        "tokens = re.findall(r'\\b\\w+\\b', data_1)\n",
        "\n",
        "def simple_stem(word):\n",
        "    return re.sub(r'(ing|ed|ly|es|s)$', '', word)\n",
        "\n",
        "stemmed_tokens = [simple_stem(token.lower()) for token in tokens]\n",
        "end_time = time.time()\n",
        "\n",
        "print_in_chunks(stemmed_tokens)\n",
        "\n",
        "print(f\"Time taken: {(end_time - start_time) * 1000000:.2f} µs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c172c2ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c172c2ff",
        "outputId": "42e97c84-80b6-41b2-ba38-08313d1a7cb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q2 PorterStemmer function\n",
            "['classif', 'is', 'the', 'task', 'of']\n",
            "['choos', 'the', 'correct', 'class', 'label']\n",
            "['for', 'a', 'given', 'input', '.']\n",
            "['in', 'basic', 'classif', 'task', ',']\n",
            "['each', 'input', 'is', 'consid', 'in']\n",
            "['isol', 'from', 'all', 'other', 'input']\n",
            "[',', 'and', 'the', 'set', 'of']\n",
            "['label', 'is', 'defin', 'in', 'advanc']\n",
            "['.', 'the', 'basic', 'classif', 'task']\n",
            "['ha', 'a', 'number', 'of', 'interest']\n",
            "['variant', '.', 'for', 'exampl', ',']\n",
            "['in', 'multiclass', 'classif', ',', 'each']\n",
            "['instanc', 'may', 'be', 'assign', 'multipl']\n",
            "['label', ';', 'in', 'open-class', 'classif']\n",
            "[',', 'the', 'set', 'of', 'label']\n",
            "['is', 'not', 'defin', 'in', 'advanc']\n",
            "[';', 'and', 'in', 'sequenc', 'classif']\n",
            "[',', 'a', 'list', 'of', 'input']\n",
            "['are', 'jointli', 'classifi', '.']\n",
            "Time taken: 2041.58 ms\n"
          ]
        }
      ],
      "source": [
        "# NLTK PorterStemmer function - Miyoko Pang\n",
        "print('Q2 PorterStemmer function')\n",
        "start = time.time()\n",
        "ps = PorterStemmer()\n",
        "porter_stems = [ps.stem(w) for w in word_tokenize(data_1)]\n",
        "end = time.time()\n",
        "print_in_chunks(porter_stems, 5)\n",
        "print(f\"Time taken: {(end - start) * 1000000:.2f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e27d1a4a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e27d1a4a",
        "outputId": "ecef2a1d-7bb8-4455-e0e8-ceb7b7fae2d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q2 LancasterStemmer function\n",
            "['class', 'is', 'the', 'task', 'of']\n",
            "['choos', 'the', 'correct', 'class', 'label']\n",
            "['for', 'a', 'giv', 'input', '.']\n",
            "['in', 'bas', 'class', 'task', ',']\n",
            "['each', 'input', 'is', 'consid', 'in']\n",
            "['isol', 'from', 'al', 'oth', 'input']\n",
            "[',', 'and', 'the', 'set', 'of']\n",
            "['label', 'is', 'defin', 'in', 'adv']\n",
            "['.', 'the', 'bas', 'class', 'task']\n",
            "['has', 'a', 'numb', 'of', 'interest']\n",
            "['vary', '.', 'for', 'exampl', ',']\n",
            "['in', 'multiclass', 'class', ',', 'each']\n",
            "['inst', 'may', 'be', 'assign', 'multipl']\n",
            "['label', ';', 'in', 'open-class', 'class']\n",
            "[',', 'the', 'set', 'of', 'label']\n",
            "['is', 'not', 'defin', 'in', 'adv']\n",
            "[';', 'and', 'in', 'sequ', 'class']\n",
            "[',', 'a', 'list', 'of', 'input']\n",
            "['ar', 'joint', 'class', '.']\n",
            "Time taken: 2579.45 ms\n"
          ]
        }
      ],
      "source": [
        "# NLTK LancasterStemmer function - Shu Hui\n",
        "print('Q2 LancasterStemmer function')\n",
        "start = time.time()\n",
        "ls = LancasterStemmer()\n",
        "lancaster_stems = [ls.stem(w) for w in word_tokenize(data_1)]\n",
        "end = time.time()\n",
        "print_in_chunks(lancaster_stems, 5)\n",
        "print(f\"Time taken: {(end - start) * 1000000:.2f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "77756c96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77756c96",
        "outputId": "6c67b891-26b5-40b3-9c0a-98be6feb6ed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# Data_2.txt Import\n",
        "# -----------------------------\n",
        "with open(\"TXSA/Data_2.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    data_2 = file.read()\n",
        "\n",
        "print('-----------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c361bf69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c361bf69",
        "outputId": "66d5fec4-0ffe-49f0-93c6-a50e8dc3cbd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q3 NLTK POS Tagger\n",
            "[('The', 'DT'), ('big', 'JJ'), ('black', 'JJ'), ('dog', 'NN'), ('barked', 'VBD')]\n",
            "[('at', 'IN'), ('the', 'DT'), ('white', 'JJ'), ('cat', 'NN'), ('and', 'CC')]\n",
            "[('chased', 'VBD'), ('away', 'RB'), ('.', '.')]\n",
            "Time taken: 417118.07 ms\n"
          ]
        }
      ],
      "source": [
        "# Q3 POS Taggers and Syntactic Analysers\n",
        "# NLTK POS tagger function - Shu Hui\n",
        "print('Q3 NLTK POS Tagger')\n",
        "start = time.time()\n",
        "pos_nltk = pos_tag(word_tokenize(data_2))\n",
        "end = time.time()\n",
        "print_in_chunks(pos_nltk, 5)\n",
        "print(f\"Time taken: {(end - start) * 1000000:.2f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "bba342d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bba342d8",
        "outputId": "7e5d313b-b97d-4761-e0d0-609ec4d5c6d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q3 TextBlob POS Tagger\n",
            "[('The', 'DT'), ('big', 'JJ'), ('black', 'JJ'), ('dog', 'NN'), ('barked', 'VBD')]\n",
            "[('at', 'IN'), ('the', 'DT'), ('white', 'JJ'), ('cat', 'NN'), ('and', 'CC')]\n",
            "[('chased', 'VBD'), ('away', 'RB')]\n",
            "Time taken: 1137.02 ms\n"
          ]
        }
      ],
      "source": [
        "# TextBlob POS Tagger - Miyoko Pang\n",
        "print('Q3 TextBlob POS Tagger')\n",
        "start = time.time()\n",
        "blob = TextBlob(data_2)\n",
        "blob_pos = blob.tags\n",
        "end = time.time()\n",
        "print_in_chunks(blob_pos, 5)\n",
        "print(f\"Time taken: {(end - start) * 1000000:.2f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "916d7674",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "916d7674",
        "outputId": "6e670243-8974-41e0-c547-fee2a281be77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q3 Regular Expression POS Tagger\n",
            "The        => NN\n",
            "big        => NN\n",
            "black      => NN\n",
            "dog        => NN\n",
            "barked     => VBD\n",
            "at         => IN\n",
            "the        => DT\n",
            "white      => NN\n",
            "cat        => NN\n",
            "and        => CC\n",
            "chased     => VBD\n",
            "away       => IN\n",
            ".          => NN\n",
            "\n",
            "Time taken: 1271.96 ms\n"
          ]
        }
      ],
      "source": [
        "# Regular Expression tagger - Yi Jing\n",
        "print('Q3 Regular Expression POS Tagger')\n",
        "start_time = time.time()\n",
        "tokens = word_tokenize(data_2)\n",
        "\n",
        "patterns = [\n",
        "    (r'.*ing$', 'VBG'),                 # gerunds\n",
        "    (r'.*ed$', 'VBD'),                  # past tense verbs\n",
        "    (r'.*es$', 'VBZ'),                  # 3rd person singular present\n",
        "    (r'.*ould$', 'MD'),                 # modals\n",
        "    (r'.*\\'s$', 'NN$'),                 # possessive nouns\n",
        "    (r'.*s$', 'NNS'),                   # plural nouns\n",
        "    (r'^-?[0-9]+(\\.[0-9]+)?$', 'CD'),   # numbers\n",
        "    (r'^(the|a|an)$', 'DT'),            # articles\n",
        "    (r'^(and|or|but)$', 'CC'),          # conjunctions\n",
        "    (r'^(at|in|on|with|away)$', 'IN'),  # prepositions\n",
        "    (r'.*', 'NN')                       # default\n",
        "]\n",
        "\n",
        "regexp_tagger = RegexpTagger(patterns)\n",
        "tagged = regexp_tagger.tag(tokens)\n",
        "end_time = time.time()\n",
        "\n",
        "for word, tag in tagged:\n",
        "    print(f\"{word:<10} => {tag}\")\n",
        "print(f\"\\nTime taken: {(end_time - start_time) * 1000000:.2f} ms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "07d3df2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "07d3df2d",
        "outputId": "90c5e154-7c9b-40cf-d5c5-2596c09117cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['the', 'big', 'black', 'dog', 'barked', 'at', 'the', 'white', 'cat', 'and', 'chased', 'away']\n",
            "                          S                                    \n",
            "      ____________________|____________                         \n",
            "     |                                 VP                      \n",
            "     |               __________________|____________________    \n",
            "     |              |             PP            |     |     |  \n",
            "     |              |      _______|____         |     |     |   \n",
            "     NP             |     |            NP       |     |     |  \n",
            "  ___|_________     |     |    ________|____    |     |     |   \n",
            "Det Adj  Adj   N    V     P  Det      Adj   N  Conj   V    Adv \n",
            " |   |    |    |    |     |   |        |    |   |     |     |   \n",
            "the big black dog barked  at the     white cat and  chased away\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('NP', [Tree('Det', ['the']), Tree('Adj', ['big']), Tree('Adj', ['black']), Tree('N', ['dog'])]), Tree('VP', [Tree('V', ['barked']), Tree('PP', [Tree('P', ['at']), Tree('NP', [Tree('Det', ['the']), Tree('Adj', ['white']), Tree('N', ['cat'])])]), Tree('Conj', ['and']), Tree('V', ['chased']), Tree('Adv', ['away'])])])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"264px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,568.0,264.0\" width=\"568px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"30.9859%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"22.7273%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Det</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.3636%\" y1=\"20px\" y2=\"48px\" /><svg width=\"22.7273%\" x=\"22.7273%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Adj</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">big</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"34.0909%\" y1=\"20px\" y2=\"48px\" /><svg width=\"31.8182%\" x=\"45.4545%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Adj</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">black</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"61.3636%\" y1=\"20px\" y2=\"48px\" /><svg width=\"22.7273%\" x=\"77.2727%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">N</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">dog</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"88.6364%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.493%\" y1=\"20px\" y2=\"48px\" /><svg width=\"69.0141%\" x=\"30.9859%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VP</text></svg><svg width=\"16.3265%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">V</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">barked</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"8.16327%\" y1=\"20px\" y2=\"48px\" /><svg width=\"42.8571%\" x=\"16.3265%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PP</text></svg><svg width=\"19.0476%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">P</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">at</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"9.52381%\" y1=\"20px\" y2=\"48px\" /><svg width=\"80.9524%\" x=\"19.0476%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"29.4118%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Det</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.7059%\" y1=\"20px\" y2=\"48px\" /><svg width=\"41.1765%\" x=\"29.4118%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Adj</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">white</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /><svg width=\"29.4118%\" x=\"70.5882%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">N</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">cat</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"85.2941%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"59.5238%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"37.7551%\" y1=\"20px\" y2=\"48px\" /><svg width=\"12.2449%\" x=\"59.1837%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Conj</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.3061%\" y1=\"20px\" y2=\"48px\" /><svg width=\"16.3265%\" x=\"71.4286%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">V</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">chased</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.5918%\" y1=\"20px\" y2=\"48px\" /><svg width=\"12.2449%\" x=\"87.7551%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Adv</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">away</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"93.8776%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.493%\" y1=\"20px\" y2=\"48px\" /></svg>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Time taken to generate parse tree: 1540.66 ms\n",
            "-----------------------------\n"
          ]
        }
      ],
      "source": [
        "# Possible Parse Trees - Miyoko and Yi Jing\n",
        "sentence = re.sub(r'[^\\w\\s]', '', data_2).lower().split()\n",
        "print(\"Tokens:\", sentence)\n",
        "grammar = CFG.fromstring(\"\"\"\n",
        "  S -> NP VP\n",
        "  NP -> Det Adj Adj N\n",
        "  NP -> Det Adj N\n",
        "  VP -> V PP Conj V Adv\n",
        "  PP -> P NP\n",
        "  Det -> 'the'\n",
        "  Adj -> 'big' | 'black' | 'white'\n",
        "  N -> 'dog' | 'cat'\n",
        "  V -> 'barked' | 'chased'\n",
        "  P -> 'at'\n",
        "  Conj -> 'and'\n",
        "  Adv -> 'away'\n",
        "\"\"\")\n",
        "parser = ChartParser(grammar)\n",
        "start = time.time()\n",
        "trees = list(parser.parse(sentence))\n",
        "end = time.time()\n",
        "if not trees:\n",
        "    print(\"No valid parse tree could be generated.\")\n",
        "else:\n",
        "    for tree in trees:\n",
        "        tree.pretty_print()\n",
        "        #tree.draw()\n",
        "        display(tree)\n",
        "print(f\"\\nTime taken to generate parse tree: {(end - start) * 1000000:.2f} ms\")\n",
        "\n",
        "print('-----------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "99485069",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99485069",
        "outputId": "eef526ef-f095-4c66-e420-14204a634d2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# Data_3.txt Import\n",
        "# -----------------------------\n",
        "with open(\"TXSA/Data_3.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    data_3 = file.read()\n",
        "\n",
        "print('-----------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "b0bdae94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0bdae94",
        "outputId": "b654ad69-817e-4b2f-9b71-0f9d68068011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q4 Unsmoothed and Smoothed Bigram Model\n",
            "Bigram (<s>, I): MLE=0.1666666667, Laplace=0.1176470588\n",
            "Bigram (I, read): MLE=1.0000000000, Laplace=0.1666666667\n",
            "Bigram (read, a): MLE=1.0000000000, Laplace=0.2857142857\n",
            "Bigram (a, different): MLE=0.3333333333, Laplace=0.1428571429\n",
            "Bigram (different, book): MLE=1.0000000000, Laplace=0.1666666667\n",
            "Bigram (book, by): MLE=0.3333333333, Laplace=0.1428571429\n",
            "Bigram (by, Danielle): MLE=1.0000000000, Laplace=0.1666666667\n",
            "Bigram (Danielle, </s>): MLE=1.0000000000, Laplace=0.1666666667\n",
            "Test Sentence: <s> I read a different book by Danielle </s>\n",
            "MLE Bigram Probability: 0.0185185185\n",
            "Laplace-smoothed Bigram Probability: 0.0000005293\n",
            "\n",
            "Time taken to generate bigram probabilities: 2.70 ms\n",
            "-----------------------------\n"
          ]
        }
      ],
      "source": [
        "# Q4 Sentence Probabilities - Bigram Models\n",
        "print('Q4 Unsmoothed and Smoothed Bigram Model')\n",
        "all_sentences = re.findall(r\"<s>.*?</s>\", data_3) # Extract wrapped sentences\n",
        "test_sentence = all_sentences[-1]\n",
        "train_sentences = all_sentences[:-1]\n",
        "tokenized_train = [['<s>'] + s.replace('<s>', '').replace('</s>', '').strip().split() + ['</s>'] for s in train_sentences]\n",
        "\n",
        "# Bigram Models (MLE and Laplace)\n",
        "start = time.time()\n",
        "n = 2\n",
        "train_data_mle, padded_sents_mle = padded_everygram_pipeline(n, tokenized_train)\n",
        "train_data_laplace, padded_sents_laplace = padded_everygram_pipeline(n, tokenized_train)\n",
        "mle_model = MLE(n)\n",
        "laplace_model = Laplace(n)\n",
        "mle_model.fit(train_data_mle, padded_sents_mle)\n",
        "laplace_model.fit(train_data_laplace, padded_sents_laplace)\n",
        "test_tokens = ['<s>'] + test_sentence.replace('<s>', '').replace('</s>', '').strip().split() + ['</s>']\n",
        "test_ngrams = list(ngrams(test_tokens, n))\n",
        "\n",
        "# Calculate probabilities\n",
        "prob_mle = 1.0\n",
        "prob_laplace = 1.0\n",
        "for w1, w2 in test_ngrams:\n",
        "    mle_score = mle_model.score(w2, [w1])\n",
        "    laplace_score = laplace_model.score(w2, [w1])\n",
        "    print(f\"Bigram ({w1}, {w2}): MLE={mle_score:.10f}, Laplace={laplace_score:.10f}\")  # Debug line\n",
        "    prob_mle *= mle_score\n",
        "    prob_laplace *= laplace_score\n",
        "end = time.time()\n",
        "print(f\"Test Sentence: {' '.join(test_tokens)}\")\n",
        "print(f\"MLE Bigram Probability: {prob_mle:.10f}\")\n",
        "print(f\"Laplace-smoothed Bigram Probability: {prob_laplace:.10f}\")\n",
        "print(f\"\\nTime taken to generate bigram probabilities: {(end - start) * 1000:.2f} ms\")\n",
        "\n",
        "print('-----------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ce9a7e30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce9a7e30",
        "outputId": "0301021b-a4ef-4eaa-a37c-5ee4dbd0ee37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q5 TreebankWordTokenizer function\n",
            "['Classification', 'is', 'the', 'task', 'of']\n",
            "['choosing', 'the', 'correct', 'class', 'label']\n",
            "['for', 'a', 'given', 'input.', 'In']\n",
            "['basic', 'classification', 'tasks', ',', 'each']\n",
            "['input', 'is', 'considered', 'in', 'isolation']\n",
            "['from', 'all', 'other', 'inputs', ',']\n",
            "['and', 'the', 'set', 'of', 'labels']\n",
            "['is', 'defined', 'in', 'advance.', 'The']\n",
            "['basic', 'classification', 'task', 'has', 'a']\n",
            "['number', 'of', 'interesting', 'variants.', 'For']\n",
            "['example', ',', 'in', 'multiclass', 'classification']\n",
            "[',', 'each', 'instance', 'may', 'be']\n",
            "['assigned', 'multiple', 'labels', ';', 'in']\n",
            "['open-class', 'classification', ',', 'the', 'set']\n",
            "['of', 'labels', 'is', 'not', 'defined']\n",
            "['in', 'advance', ';', 'and', 'in']\n",
            "['sequence', 'classification', ',', 'a', 'list']\n",
            "['of', 'inputs', 'are', 'jointly', 'classified']\n",
            "['.']\n",
            "Time taken: 602.01 ms\n"
          ]
        }
      ],
      "source": [
        "# Q5 Individual Work Assignments\n",
        "# Miyoko Pang\n",
        "print('Q5 TreebankWordTokenizer function')\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "data_1_clean = data_1.replace('\\n', ' ').strip()\n",
        "start = time.time()\n",
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "tokens = treebank_tokenizer.tokenize(data_1_clean)\n",
        "end = time.time()\n",
        "print_in_chunks(tokens, 5)\n",
        "print(f\"Time taken: {(end - start) * 1000000:.2f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "94d949e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94d949e0",
        "outputId": "307668dd-1464-4c54-8eb6-4071137772fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q5 spaCy Tokenizer\n",
            "['Classification', 'is', 'the', 'task', 'of']\n",
            "['choosing', 'the', 'correct', 'class', 'label']\n",
            "['for', 'a', 'given', 'input', '.']\n",
            "['In', 'basic', 'classification', 'tasks', ',']\n",
            "['each', 'input', 'is', 'considered', 'in']\n",
            "['isolation', 'from', 'all', 'other', 'inputs']\n",
            "[',', 'and', 'the', 'set', 'of']\n",
            "['labels', 'is', 'defined', 'in', 'advance']\n",
            "['.', 'The', 'basic', 'classification', 'task']\n",
            "['has', 'a', 'number', 'of', 'interesting']\n",
            "['variants', '.', 'For', 'example', ',']\n",
            "['in', 'multiclass', 'classification', ',', 'each']\n",
            "['instance', 'may', 'be', 'assigned', 'multiple']\n",
            "['labels', ';', 'in', 'open', '-']\n",
            "['class', 'classification', ',', 'the', 'set']\n",
            "['of', 'labels', 'is', 'not', 'defined']\n",
            "['in', 'advance', ';', 'and', 'in']\n",
            "['sequence', 'classification', ',', 'a', 'list']\n",
            "['of', 'inputs', 'are', 'jointly', 'classified']\n",
            "['.']\n",
            "\n",
            "Total Tokens: 96\n",
            "Time taken: 44330.12 µs\n"
          ]
        }
      ],
      "source": [
        "# Yi Jing\n",
        "print(\"Q5 spaCy Tokenizer\")\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Clean newline and extra whitespace\n",
        "clean_data_1 = re.sub(r'\\s+', ' ', data_1.strip())\n",
        "\n",
        "start = time.time()\n",
        "doc = nlp(clean_data_1)\n",
        "tokens = [token.text for token in doc]\n",
        "end = time.time()\n",
        "\n",
        "print_in_chunks(tokens)\n",
        "print(f\"\\nTotal Tokens: {len(tokens)}\")\n",
        "print(f\"Time taken: {(end - start) * 1000000:.2f} µs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "5516b3c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5516b3c9",
        "outputId": "ca35935c-36a1-45b8-e55a-f81f494114f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q5 Alternative Tokenizer - WordPunctTokenizer\n",
            "['Classification', 'is', 'the', 'task', 'of']\n",
            "['choosing', 'the', 'correct', 'class', 'label']\n",
            "['for', 'a', 'given', 'input', '.']\n",
            "['In', 'basic', 'classification', 'tasks', ',']\n",
            "['each', 'input', 'is', 'considered', 'in']\n",
            "['isolation', 'from', 'all', 'other', 'inputs']\n",
            "[',', 'and', 'the', 'set', 'of']\n",
            "['labels', 'is', 'defined', 'in', 'advance']\n",
            "['.', 'The', 'basic', 'classification', 'task']\n",
            "['has', 'a', 'number', 'of', 'interesting']\n",
            "['variants', '.', 'For', 'example', ',']\n",
            "['in', 'multiclass', 'classification', ',', 'each']\n",
            "['instance', 'may', 'be', 'assigned', 'multiple']\n",
            "['labels', ';', 'in', 'open', '-']\n",
            "['class', 'classification', ',', 'the', 'set']\n",
            "['of', 'labels', 'is', 'not', 'defined']\n",
            "['in', 'advance', ';', 'and', 'in']\n",
            "['sequence', 'classification', ',', 'a', 'list']\n",
            "['of', 'inputs', 'are', 'jointly', 'classified']\n",
            "['.']\n",
            "Time taken: 321.15 ms\n"
          ]
        }
      ],
      "source": [
        "# Shu Hui\n",
        "print('Q5 Alternative Tokenizer - WordPunctTokenizer')\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "import time\n",
        "\n",
        "# Clean the data\n",
        "data_1_clean2 = data_1.replace('\\n', ' ').strip()\n",
        "\n",
        "start = time.time()\n",
        "wordpunct_tokenizer = WordPunctTokenizer()\n",
        "tokens = wordpunct_tokenizer.tokenize(data_1_clean2)\n",
        "end = time.time()\n",
        "\n",
        "print_in_chunks(tokens, 5)\n",
        "print(f\"Time taken: {(end - start) * 1000000:.2f} ms\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}