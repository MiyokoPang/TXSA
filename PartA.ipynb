{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5041354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import pprint\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk import pos_tag, FreqDist, ConditionalFreqDist, CFG\n",
    "from nltk.util import bigrams\n",
    "from nltk.tag import RegexpTagger\n",
    "from nltk.probability import ConditionalProbDist, MLEProbDist, LidstoneProbDist\n",
    "from nltk.parse.chart import ChartParser\n",
    "from nltk.lm import MLE, Laplace\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b129150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Function for cleaner display format - Miyoko\n",
    "def print_in_chunks(token_list, chunk_size=5):\n",
    "    for i in range(0, len(token_list), chunk_size):\n",
    "        print(token_list[i:i+chunk_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e871458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Data_1.txt Import\n",
    "# -----------------------------\n",
    "with open(\"Data_1.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data_1 = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca62a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 Word Tokenization\n",
    "# split() function - Miyoko Pang\n",
    "print('Q1 split function')\n",
    "start = time.time()\n",
    "tokenization_1 = data_1.split()\n",
    "end = time.time()\n",
    "print_in_chunks(tokenization_1, 5)\n",
    "print(f\"Time taken: {(end - start) * 1000000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f0c6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Expression function - Yi Jing\n",
    "print('Q1 RE function')\n",
    "start_time = time.time()\n",
    "tokens = re.findall(r'\\b\\w+\\b', data_1)\n",
    "end_time = time.time()\n",
    "\n",
    "print_in_chunks(tokens)\n",
    "print(f\"\\nTotal number of tokens: {len(tokens)}\")\n",
    "print(f\"Time taken: {(end_time - start_time) * 1000000:.2f} µs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fd3ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK function - Shu Hui\n",
    "print('Q1 NLTK function')\n",
    "start = time.time()\n",
    "tokenization = word_tokenize(data_1)\n",
    "end = time.time()\n",
    "print_in_chunks(tokenization, 5)\n",
    "print(f\"Time taken: {(end - start) * 1000000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d1c7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.3: Stop Words & Punctuation Removal - Yi Jing\n",
    "print(\"Q1.3 Stop Words & Punctuation Removal\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(data_1)\n",
    "\n",
    "# Prepare stop words and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "# Remove stop words and punctuation\n",
    "filtered_tokens = []\n",
    "found_stopwords = []\n",
    "\n",
    "for word in tokens:\n",
    "    word_lower = word.lower()\n",
    "    if word_lower in stop_words:\n",
    "        found_stopwords.append(word_lower)\n",
    "    elif word not in punctuation:\n",
    "        filtered_tokens.append(word)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Print results\n",
    "print(\"Original Token Count:\", len(tokens))\n",
    "print(\"Filtered Token Count:\", len(filtered_tokens))\n",
    "\n",
    "print(\"\\nFiltered Tokens:\")\n",
    "print_in_chunks(filtered_tokens)\n",
    "\n",
    "print(\"\\nStop Words Found and Removed:\")\n",
    "print_in_chunks(found_stopwords)\n",
    "\n",
    "print(f\"\\nTotal Stop Words Removed: {len(found_stopwords)}\")\n",
    "print(f\"Time taken: {(end_time - start_time) * 1000000:.2f} µs\")\n",
    "\n",
    "print('-----------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a0a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 Form Word Stemming\n",
    "# Regular Expression function - Yi Jing\n",
    "print('Q2 RE function')\n",
    "start_time = time.time()\n",
    "tokens = re.findall(r'\\b\\w+\\b', data_1)\n",
    "\n",
    "def simple_stem(word):\n",
    "    return re.sub(r'(ing|ed|ly|es|s)$', '', word)\n",
    "\n",
    "stemmed_tokens = [simple_stem(token.lower()) for token in tokens]\n",
    "end_time = time.time()\n",
    "\n",
    "print_in_chunks(stemmed_tokens)\n",
    "\n",
    "print(f\"Time taken: {(end_time - start_time) * 1000000:.2f} µs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c172c2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK PorterStemmer function - Miyoko Pang\n",
    "print('Q2 PorterStemmer function')\n",
    "start = time.time()\n",
    "ps = PorterStemmer()\n",
    "porter_stems = [ps.stem(w) for w in word_tokenize(data_1)]\n",
    "end = time.time()\n",
    "print_in_chunks(porter_stems, 5)\n",
    "print(f\"Time taken: {(end - start) * 1000000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27d1a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK LancasterStemmer function - Shu Hui\n",
    "print('Q2 LancasterStemmer function')\n",
    "start = time.time()\n",
    "ls = LancasterStemmer()\n",
    "lancaster_stems = [ls.stem(w) for w in word_tokenize(data_1)]\n",
    "end = time.time()\n",
    "print_in_chunks(lancaster_stems, 5)\n",
    "print(f\"Time taken: {(end - start) * 1000000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77756c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Data_2.txt Import\n",
    "# -----------------------------\n",
    "with open(\"Data_2.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data_2 = file.read()\n",
    "\n",
    "print('-----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c361bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 POS Taggers and Syntactic Analysers\n",
    "# NLTK POS tagger function - Shu Hui\n",
    "print('Q3 NLTK POS Tagger')\n",
    "start = time.time()\n",
    "pos_nltk = pos_tag(word_tokenize(data_2))\n",
    "end = time.time()\n",
    "print_in_chunks(pos_nltk, 5)\n",
    "print(f\"Time taken: {(end - start) * 1000000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba342d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextBlob POS Tagger - Miyoko Pang\n",
    "print('Q3 TextBlob POS Tagger')\n",
    "start = time.time()\n",
    "blob = TextBlob(data_2)\n",
    "blob_pos = blob.tags\n",
    "end = time.time()\n",
    "print_in_chunks(blob_pos, 5)\n",
    "print(f\"Time taken: {(end - start) * 1000000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d7674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Expression tagger - Yi Jing\n",
    "print('Q3 Regular Expression POS Tagger')\n",
    "start_time = time.time()\n",
    "tokens = word_tokenize(data_2)\n",
    "\n",
    "patterns = [\n",
    "    (r'.*ing$', 'VBG'),                 # gerunds\n",
    "    (r'.*ed$', 'VBD'),                  # past tense verbs\n",
    "    (r'.*es$', 'VBZ'),                  # 3rd person singular present\n",
    "    (r'.*ould$', 'MD'),                 # modals\n",
    "    (r'.*\\'s$', 'NN$'),                 # possessive nouns\n",
    "    (r'.*s$', 'NNS'),                   # plural nouns\n",
    "    (r'^-?[0-9]+(\\.[0-9]+)?$', 'CD'),   # numbers\n",
    "    (r'^(the|a|an)$', 'DT'),            # articles\n",
    "    (r'^(and|or|but)$', 'CC'),          # conjunctions\n",
    "    (r'^(at|in|on|with|away)$', 'IN'),  # prepositions\n",
    "    (r'.*', 'NN')                       # default\n",
    "]\n",
    "\n",
    "regexp_tagger = RegexpTagger(patterns)\n",
    "tagged = regexp_tagger.tag(tokens)\n",
    "end_time = time.time()\n",
    "\n",
    "for word, tag in tagged:\n",
    "    print(f\"{word:<10} => {tag}\")\n",
    "print(f\"\\nTime taken: {(end_time - start_time) * 1000000:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d3df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible Parse Trees - Miyoko and Yi Jing\n",
    "sentence = re.sub(r'[^\\w\\s]', '', data_2).lower().split()\n",
    "print(\"Tokens:\", sentence)\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  NP -> Det Adj Adj N\n",
    "  NP -> Det Adj N\n",
    "  VP -> V PP Conj V Adv\n",
    "  PP -> P NP\n",
    "  Det -> 'the'\n",
    "  Adj -> 'big' | 'black' | 'white'\n",
    "  N -> 'dog' | 'cat'\n",
    "  V -> 'barked' | 'chased'\n",
    "  P -> 'at'\n",
    "  Conj -> 'and'\n",
    "  Adv -> 'away'\n",
    "\"\"\")\n",
    "parser = ChartParser(grammar)\n",
    "start = time.time()\n",
    "trees = list(parser.parse(sentence))\n",
    "end = time.time()\n",
    "if not trees:\n",
    "    print(\"No valid parse tree could be generated.\")\n",
    "else:\n",
    "    for tree in trees:\n",
    "        tree.pretty_print()\n",
    "        tree.draw()\n",
    "print(f\"\\nTime taken to generate parse tree: {(end - start) * 1000000:.2f} ms\")\n",
    "\n",
    "print('-----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99485069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Data_3.txt Import\n",
    "# -----------------------------\n",
    "with open(\"Data_3.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data_3 = file.read()\n",
    "\n",
    "print('-----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bdae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 Sentence Probabilities - Bigram Models\n",
    "print('Q4 Unsmoothed and Smoothed Bigram Model')\n",
    "all_sentences = re.findall(r\"<s>.*?</s>\", data_3) # Extract wrapped sentences\n",
    "test_sentence = all_sentences[-1]\n",
    "train_sentences = all_sentences[:-1]\n",
    "tokenized_train = [['<s>'] + s.replace('<s>', '').replace('</s>', '').strip().split() + ['</s>'] for s in train_sentences]\n",
    "\n",
    "# Bigram Models (MLE and Laplace)\n",
    "start = time.time()\n",
    "n = 2\n",
    "train_data_mle, padded_sents_mle = padded_everygram_pipeline(n, tokenized_train)\n",
    "train_data_laplace, padded_sents_laplace = padded_everygram_pipeline(n, tokenized_train)\n",
    "mle_model = MLE(n)\n",
    "laplace_model = Laplace(n)\n",
    "mle_model.fit(train_data_mle, padded_sents_mle)\n",
    "laplace_model.fit(train_data_laplace, padded_sents_laplace)\n",
    "test_tokens = ['<s>'] + test_sentence.replace('<s>', '').replace('</s>', '').strip().split() + ['</s>']\n",
    "test_ngrams = list(ngrams(test_tokens, n))\n",
    "\n",
    "# Calculate probabilities\n",
    "prob_mle = 1.0\n",
    "prob_laplace = 1.0\n",
    "for w1, w2 in test_ngrams:\n",
    "    mle_score = mle_model.score(w2, [w1])\n",
    "    laplace_score = laplace_model.score(w2, [w1])\n",
    "    print(f\"Bigram ({w1}, {w2}): MLE={mle_score:.10f}, Laplace={laplace_score:.10f}\")  # Debug line\n",
    "    prob_mle *= mle_score\n",
    "    prob_laplace *= laplace_score\n",
    "end = time.time()\n",
    "print(f\"Test Sentence: {' '.join(test_tokens)}\")\n",
    "print(f\"MLE Bigram Probability: {prob_mle:.10f}\")\n",
    "print(f\"Laplace-smoothed Bigram Probability: {prob_laplace:.10f}\")\n",
    "print(f\"\\nTime taken to generate bigram probabilities: {(end - start) * 1000:.2f} ms\")\n",
    "\n",
    "print('-----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9a7e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5 Individual Work Assignments\n",
    "# Miyoko Pang\n",
    "print('Q5 TreebankWordTokenizer function')\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "data_1_clean = data_1.replace('\\n', ' ').strip()\n",
    "start = time.time()\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "tokens = treebank_tokenizer.tokenize(data_1_clean)\n",
    "end = time.time()\n",
    "print_in_chunks(tokens, 5)\n",
    "print(f\"Time taken: {(end - start) * 1000000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d949e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yi Jing\n",
    "print(\"Q5 spaCy Tokenizer\")\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Clean newline and extra whitespace\n",
    "clean_data_1 = re.sub(r'\\s+', ' ', data_1.strip())\n",
    "\n",
    "start = time.time()\n",
    "doc = nlp(clean_data_1)\n",
    "tokens = [token.text for token in doc]\n",
    "end = time.time()\n",
    "\n",
    "print_in_chunks(tokens)\n",
    "print(f\"\\nTotal Tokens: {len(tokens)}\")\n",
    "print(f\"Time taken: {(end - start) * 1000000:.2f} µs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5516b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shu Hui     \n",
    "print('Q5 Alternative Tokenizer - WordPunctTokenizer')\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import time\n",
    "\n",
    "# Clean the data\n",
    "data_1_clean2 = data_1.replace('\\n', ' ').strip()\n",
    "\n",
    "start = time.time()\n",
    "wordpunct_tokenizer = WordPunctTokenizer()\n",
    "tokens = wordpunct_tokenizer.tokenize(data_1_clean2)\n",
    "end = time.time()\n",
    "\n",
    "print_in_chunks(tokens, 5)\n",
    "print(f\"Time taken: {(end - start) * 1000000:.2f} ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
