{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MiyokoPang/TXSA/blob/main/PartA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5041354",
      "metadata": {
        "id": "a5041354"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import json\n",
        "import pprint\n",
        "from pprint import pprint\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "from nltk import pos_tag, FreqDist, ConditionalFreqDist, CFG\n",
        "from nltk.util import bigrams\n",
        "from nltk.tag import RegexpTagger\n",
        "from nltk.probability import ConditionalProbDist, MLEProbDist, LidstoneProbDist\n",
        "from nltk.parse.chart import ChartParser\n",
        "from nltk.lm import MLE, Laplace\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.util import ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b129150",
      "metadata": {
        "id": "6b129150"
      },
      "outputs": [],
      "source": [
        "# Downloads\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Function for cleaner display format - Miyoko\n",
        "def print_in_chunks(token_list, chunk_size=5):\n",
        "    for i in range(0, len(token_list), chunk_size):\n",
        "        print(token_list[i:i+chunk_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e871458b",
      "metadata": {
        "id": "e871458b"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Data_1.txt Import\n",
        "# -----------------------------\n",
        "!git clone https://github.com/MiyokoPang/TXSA.git\n",
        "with open(\"TXSA/Data_1.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    data_1 = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ca62a84",
      "metadata": {
        "id": "2ca62a84"
      },
      "outputs": [],
      "source": [
        "# Q1 Word Tokenization\n",
        "# split() function - Miyoko Pang\n",
        "print('Q1 split function')\n",
        "start = time.time()\n",
        "tokenization_1 = data_1.split()\n",
        "end = time.time()\n",
        "print_in_chunks(tokenization_1, 5)\n",
        "print(f\"Time taken: {(end - start) * 1000000:.2f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8f0c6b7",
      "metadata": {
        "id": "f8f0c6b7"
      },
      "outputs": [],
      "source": [
        "# Regular Expression function - Yi Jing\n",
        "print('Q1 RE function')\n",
        "start_time = time.time()\n",
        "tokens = re.findall(r'\\b\\w+\\b', data_1)\n",
        "end_time = time.time()\n",
        "\n",
        "print_in_chunks(tokens)\n",
        "print(f\"\\nTotal number of tokens: {len(tokens)}\")\n",
        "print(f\"Time taken: {(end_time - start_time) * 1000000:.2f} µs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95fd3ff7",
      "metadata": {
        "id": "95fd3ff7"
      },
      "outputs": [],
      "source": [
        "# NLTK function - Shu Hui\n",
        "print('Q1 NLTK function')\n",
        "start = time.time()\n",
        "tokenization = word_tokenize(data_1)\n",
        "end = time.time()\n",
        "print_in_chunks(tokenization, 5)\n",
        "print(f\"Time taken: {(end - start) * 1000000:.2f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0d1c7ef",
      "metadata": {
        "id": "d0d1c7ef"
      },
      "outputs": [],
      "source": [
        "# Q1.3: Stop Words & Punctuation Removal - Yi Jing\n",
        "print(\"Q1.3 Stop Words & Punctuation Removal\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Tokenize\n",
        "tokens = word_tokenize(data_1)\n",
        "\n",
        "# Prepare stop words and punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "# Remove stop words and punctuation\n",
        "filtered_tokens = []\n",
        "found_stopwords = []\n",
        "\n",
        "for word in tokens:\n",
        "    word_lower = word.lower()\n",
        "    if word_lower in stop_words:\n",
        "        found_stopwords.append(word_lower)\n",
        "    elif word not in punctuation:\n",
        "        filtered_tokens.append(word)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Print results\n",
        "print(\"Original Token Count:\", len(tokens))\n",
        "print(\"Filtered Token Count:\", len(filtered_tokens))\n",
        "\n",
        "print(\"\\nFiltered Tokens:\")\n",
        "print_in_chunks(filtered_tokens)\n",
        "\n",
        "print(\"\\nStop Words Found and Removed:\")\n",
        "print_in_chunks(found_stopwords)\n",
        "\n",
        "print(f\"\\nTotal Stop Words Removed: {len(found_stopwords)}\")\n",
        "print(f\"Time taken: {(end_time - start_time) * 1000000:.2f} µs\")\n",
        "\n",
        "print('-----------------------------')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56a0a1a7",
      "metadata": {
        "id": "56a0a1a7"
      },
      "outputs": [],
      "source": [
        "# Q2 Form Word Stemming\n",
        "# Regular Expression function - Yi Jing\n",
        "print('Q2 RE function')\n",
        "start_time = time.time()\n",
        "tokens = re.findall(r'\\b\\w+\\b', data_1)\n",
        "\n",
        "def simple_stem(word):\n",
        "    return re.sub(r'(ing|ed|ly|es|s)$', '', word)\n",
        "\n",
        "stemmed_tokens = [simple_stem(token.lower()) for token in tokens]\n",
        "end_time = time.time()\n",
        "\n",
        "print_in_chunks(stemmed_tokens)\n",
        "\n",
        "print(f\"Time taken: {(end_time - start_time) * 1000000:.2f} µs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c172c2ff",
      "metadata": {
        "id": "c172c2ff"
      },
      "outputs": [],
      "source": [
        "# NLTK PorterStemmer function - Miyoko Pang\n",
        "print('Q2 PorterStemmer function')\n",
        "start = time.time()\n",
        "ps = PorterStemmer()\n",
        "porter_stems = [ps.stem(w) for w in word_tokenize(data_1)]\n",
        "end = time.time()\n",
        "print_in_chunks(porter_stems, 5)\n",
        "print(f\"Time taken: {(end - start) * 1000000:.2f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e27d1a4a",
      "metadata": {
        "id": "e27d1a4a"
      },
      "outputs": [],
      "source": [
        "# NLTK LancasterStemmer function - Shu Hui\n",
        "print('Q2 LancasterStemmer function')\n",
        "start = time.time()\n",
        "ls = LancasterStemmer()\n",
        "lancaster_stems = [ls.stem(w) for w in word_tokenize(data_1)]\n",
        "end = time.time()\n",
        "print_in_chunks(lancaster_stems, 5)\n",
        "print(f\"Time taken: {(end - start) * 1000000:.2f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77756c96",
      "metadata": {
        "id": "77756c96"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Data_2.txt Import\n",
        "# -----------------------------\n",
        "with open(\"Data_2.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    data_2 = file.read()\n",
        "\n",
        "print('-----------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c361bf69",
      "metadata": {
        "id": "c361bf69"
      },
      "outputs": [],
      "source": [
        "# Q3 POS Taggers and Syntactic Analysers\n",
        "# NLTK POS tagger function - Shu Hui\n",
        "print('Q3 NLTK POS Tagger')\n",
        "start = time.time()\n",
        "pos_nltk = pos_tag(word_tokenize(data_2))\n",
        "end = time.time()\n",
        "print_in_chunks(pos_nltk, 5)\n",
        "print(f\"Time taken: {(end - start) * 1000000:.2f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bba342d8",
      "metadata": {
        "id": "bba342d8"
      },
      "outputs": [],
      "source": [
        "# TextBlob POS Tagger - Miyoko Pang\n",
        "print('Q3 TextBlob POS Tagger')\n",
        "start = time.time()\n",
        "blob = TextBlob(data_2)\n",
        "blob_pos = blob.tags\n",
        "end = time.time()\n",
        "print_in_chunks(blob_pos, 5)\n",
        "print(f\"Time taken: {(end - start) * 1000000:.2f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "916d7674",
      "metadata": {
        "id": "916d7674"
      },
      "outputs": [],
      "source": [
        "# Regular Expression tagger - Yi Jing\n",
        "print('Q3 Regular Expression POS Tagger')\n",
        "start_time = time.time()\n",
        "tokens = word_tokenize(data_2)\n",
        "\n",
        "patterns = [\n",
        "    (r'.*ing$', 'VBG'),                 # gerunds\n",
        "    (r'.*ed$', 'VBD'),                  # past tense verbs\n",
        "    (r'.*es$', 'VBZ'),                  # 3rd person singular present\n",
        "    (r'.*ould$', 'MD'),                 # modals\n",
        "    (r'.*\\'s$', 'NN$'),                 # possessive nouns\n",
        "    (r'.*s$', 'NNS'),                   # plural nouns\n",
        "    (r'^-?[0-9]+(\\.[0-9]+)?$', 'CD'),   # numbers\n",
        "    (r'^(the|a|an)$', 'DT'),            # articles\n",
        "    (r'^(and|or|but)$', 'CC'),          # conjunctions\n",
        "    (r'^(at|in|on|with|away)$', 'IN'),  # prepositions\n",
        "    (r'.*', 'NN')                       # default\n",
        "]\n",
        "\n",
        "regexp_tagger = RegexpTagger(patterns)\n",
        "tagged = regexp_tagger.tag(tokens)\n",
        "end_time = time.time()\n",
        "\n",
        "for word, tag in tagged:\n",
        "    print(f\"{word:<10} => {tag}\")\n",
        "print(f\"\\nTime taken: {(end_time - start_time) * 1000000:.2f} ms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07d3df2d",
      "metadata": {
        "id": "07d3df2d"
      },
      "outputs": [],
      "source": [
        "# Possible Parse Trees - Miyoko and Yi Jing\n",
        "sentence = re.sub(r'[^\\w\\s]', '', data_2).lower().split()\n",
        "print(\"Tokens:\", sentence)\n",
        "grammar = CFG.fromstring(\"\"\"\n",
        "  S -> NP VP\n",
        "  NP -> Det Adj Adj N\n",
        "  NP -> Det Adj N\n",
        "  VP -> V PP Conj V Adv\n",
        "  PP -> P NP\n",
        "  Det -> 'the'\n",
        "  Adj -> 'big' | 'black' | 'white'\n",
        "  N -> 'dog' | 'cat'\n",
        "  V -> 'barked' | 'chased'\n",
        "  P -> 'at'\n",
        "  Conj -> 'and'\n",
        "  Adv -> 'away'\n",
        "\"\"\")\n",
        "parser = ChartParser(grammar)\n",
        "start = time.time()\n",
        "trees = list(parser.parse(sentence))\n",
        "end = time.time()\n",
        "if not trees:\n",
        "    print(\"No valid parse tree could be generated.\")\n",
        "else:\n",
        "    for tree in trees:\n",
        "        tree.pretty_print()\n",
        "        tree.draw()\n",
        "print(f\"\\nTime taken to generate parse tree: {(end - start) * 1000000:.2f} ms\")\n",
        "\n",
        "print('-----------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99485069",
      "metadata": {
        "id": "99485069"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Data_3.txt Import\n",
        "# -----------------------------\n",
        "with open(\"Data_3.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    data_3 = file.read()\n",
        "\n",
        "print('-----------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0bdae94",
      "metadata": {
        "id": "b0bdae94"
      },
      "outputs": [],
      "source": [
        "# Q4 Sentence Probabilities - Bigram Models\n",
        "print('Q4 Unsmoothed and Smoothed Bigram Model')\n",
        "all_sentences = re.findall(r\"<s>.*?</s>\", data_3) # Extract wrapped sentences\n",
        "test_sentence = all_sentences[-1]\n",
        "train_sentences = all_sentences[:-1]\n",
        "tokenized_train = [['<s>'] + s.replace('<s>', '').replace('</s>', '').strip().split() + ['</s>'] for s in train_sentences]\n",
        "\n",
        "# Bigram Models (MLE and Laplace)\n",
        "start = time.time()\n",
        "n = 2\n",
        "train_data_mle, padded_sents_mle = padded_everygram_pipeline(n, tokenized_train)\n",
        "train_data_laplace, padded_sents_laplace = padded_everygram_pipeline(n, tokenized_train)\n",
        "mle_model = MLE(n)\n",
        "laplace_model = Laplace(n)\n",
        "mle_model.fit(train_data_mle, padded_sents_mle)\n",
        "laplace_model.fit(train_data_laplace, padded_sents_laplace)\n",
        "test_tokens = ['<s>'] + test_sentence.replace('<s>', '').replace('</s>', '').strip().split() + ['</s>']\n",
        "test_ngrams = list(ngrams(test_tokens, n))\n",
        "\n",
        "# Calculate probabilities\n",
        "prob_mle = 1.0\n",
        "prob_laplace = 1.0\n",
        "for w1, w2 in test_ngrams:\n",
        "    mle_score = mle_model.score(w2, [w1])\n",
        "    laplace_score = laplace_model.score(w2, [w1])\n",
        "    print(f\"Bigram ({w1}, {w2}): MLE={mle_score:.10f}, Laplace={laplace_score:.10f}\")  # Debug line\n",
        "    prob_mle *= mle_score\n",
        "    prob_laplace *= laplace_score\n",
        "end = time.time()\n",
        "print(f\"Test Sentence: {' '.join(test_tokens)}\")\n",
        "print(f\"MLE Bigram Probability: {prob_mle:.10f}\")\n",
        "print(f\"Laplace-smoothed Bigram Probability: {prob_laplace:.10f}\")\n",
        "print(f\"\\nTime taken to generate bigram probabilities: {(end - start) * 1000:.2f} ms\")\n",
        "\n",
        "print('-----------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce9a7e30",
      "metadata": {
        "id": "ce9a7e30"
      },
      "outputs": [],
      "source": [
        "# Q5 Individual Work Assignments\n",
        "# Miyoko Pang\n",
        "print('Q5 TreebankWordTokenizer function')\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "data_1_clean = data_1.replace('\\n', ' ').strip()\n",
        "start = time.time()\n",
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "tokens = treebank_tokenizer.tokenize(data_1_clean)\n",
        "end = time.time()\n",
        "print_in_chunks(tokens, 5)\n",
        "print(f\"Time taken: {(end - start) * 1000000:.2f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94d949e0",
      "metadata": {
        "id": "94d949e0"
      },
      "outputs": [],
      "source": [
        "# Yi Jing\n",
        "print(\"Q5 spaCy Tokenizer\")\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Clean newline and extra whitespace\n",
        "clean_data_1 = re.sub(r'\\s+', ' ', data_1.strip())\n",
        "\n",
        "start = time.time()\n",
        "doc = nlp(clean_data_1)\n",
        "tokens = [token.text for token in doc]\n",
        "end = time.time()\n",
        "\n",
        "print_in_chunks(tokens)\n",
        "print(f\"\\nTotal Tokens: {len(tokens)}\")\n",
        "print(f\"Time taken: {(end - start) * 1000000:.2f} µs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5516b3c9",
      "metadata": {
        "id": "5516b3c9"
      },
      "outputs": [],
      "source": [
        "# Shu Hui\n",
        "print('Q5 Alternative Tokenizer - WordPunctTokenizer')\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "import time\n",
        "\n",
        "# Clean the data\n",
        "data_1_clean2 = data_1.replace('\\n', ' ').strip()\n",
        "\n",
        "start = time.time()\n",
        "wordpunct_tokenizer = WordPunctTokenizer()\n",
        "tokens = wordpunct_tokenizer.tokenize(data_1_clean2)\n",
        "end = time.time()\n",
        "\n",
        "print_in_chunks(tokens, 5)\n",
        "print(f\"Time taken: {(end - start) * 1000000:.2f} ms\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}