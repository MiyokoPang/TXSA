{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4cabc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Q1: Preprocessing and Exploratory Data Analysis\n",
    "# -----------------------------\n",
    "# Shu Hui\n",
    "print(\"Q1: Preprocessing and Exploratory Data Analysis - Shu Hui\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from pathlib import Path\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "\n",
    "# Download required resources\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6080c2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"Loading dataset\")\n",
    "path_dir = Path(__file__).resolve().parent\n",
    "df_dir = path_dir / \"sentiment_tweets3.csv\"\n",
    "df = pd.read_csv(df_dir)\n",
    "df.columns = ['Index', 'message', 'label']\n",
    "\n",
    "# Step 1: Clean raw text \n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text.lower()\n",
    "\n",
    "print(\"Cleaning text...\")\n",
    "df['clean_message'] = df['message'].astype(str).apply(clean_text)\n",
    "\n",
    "# Step 2: Stopwords removal and stemming \n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "print(\"Preprocessing text (stopword removal + stemming)\")\n",
    "df['processed_message'] = df['clean_message'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094d92b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Class Distribution \n",
    "print(\"\\nQ1.1: Class Distribution\")\n",
    "label_counts = df['label'].value_counts()\n",
    "for label, count in label_counts.items():\n",
    "    label_name = \"Not Depressed\" if label == 0 else \"Depressed\"\n",
    "    print(f\"{label} ({label_name}): {count} tweets\")\n",
    "\n",
    "# Pie chart\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(label_counts, labels=['Not Depressed (0)', 'Depressed (1)'], autopct='%1.1f%%', startangle=90)\n",
    "plt.title(\"Tweet Label Distribution\")\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793fe690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Average Tweet Length \n",
    "print(\"\\nQ1.2: Average Tweet Length by Label\")\n",
    "df['tweet_length'] = df['message'].astype(str).apply(len)\n",
    "avg_lengths = df.groupby('label')['tweet_length'].mean()\n",
    "print(f\"Not Depressed (0): {avg_lengths[0]:.2f} characters\")\n",
    "print(f\"Depressed (1): {avg_lengths[1]:.2f} characters\")\n",
    "\n",
    "# Bar chart\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=avg_lengths.index, y=avg_lengths.values)\n",
    "plt.xticks([0, 1], ['Not Depressed', 'Depressed'])\n",
    "plt.ylabel(\"Average Tweet Length\")\n",
    "plt.title(\"Average Tweet Length by Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09df8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Top 20 Frequent Words \n",
    "print(\"\\nQ1.3: Top 20 Most Frequent Words\")\n",
    "all_words = ' '.join(df['processed_message']).split()\n",
    "common_words = Counter(all_words).most_common(20)\n",
    "\n",
    "for word, count in common_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Plot\n",
    "words = [word for word, count in common_words]\n",
    "counts = [count for word, count in common_words]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(words[::-1], counts[::-1], color='skyblue')\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Top 20 Most Frequent Words in Tweets')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5150be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Individual Section: Q2: Supervised Text Classification Model + Q3: Hyper Parameter Selection\n",
    "# -----------------------------\n",
    "\n",
    "print(\"Individual Sections: Q2: Supervised Text Classification Model + Q3: Hyper Parameter Selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29130bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miyoko Pang: Logistic Regression\n",
    "print(\"Miyoko Pang (TP067553)\")\n",
    "print(\"\\nQ2: Supervised Text Classification Model: Logistic Regression\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "X = df['processed_message']\n",
    "y = df['label']\n",
    "\n",
    "# Train-test split\n",
    "start_time = time.time()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"Data split. Time taken: {(end_time - start_time) * 1000000:.2f} µs\")\n",
    "\n",
    "# TF-IDF vectorization\n",
    "start_time = time.time()\n",
    "vectorizer = TfidfVectorizer( max_features=5000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "end_time = time.time()\n",
    "print(f\"TF-IDF vectorization complete. Time taken: {(end_time - start_time) * 1000000:.2f} µs\")\n",
    "\n",
    "# Train logistic regression\n",
    "start_time = time.time()\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_vec, y_train)\n",
    "end_time = time.time()\n",
    "print(f\"Model trained. Time taken: {(end_time - start_time) * 1000000:.2f} µs\")\n",
    "\n",
    "# Predict\n",
    "start_time = time.time()\n",
    "y_pred = model.predict(X_test_vec)\n",
    "end_time = time.time()\n",
    "print(f\"Prediction complete. Time taken: {(end_time - start_time) * 1000000:.2f} µs\")\n",
    "\n",
    "# Results\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e409b10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nQ3: Hyper Parameter Selection - Logistic Regression\")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'max_iter': [100, 200]\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "grid = GridSearchCV(LogisticRegression(), params, cv=5, scoring='f1', verbose=1)\n",
    "grid.fit(X_train_vec, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Best Params:\", grid.best_params_)\n",
    "print(f\"Grid Search Time: {(end_time - start_time) * 1000:.2f} ms\")\n",
    "\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc2b1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shu Hui: KNN\n",
    "print(\"Shu Hui\")\n",
    "print(\"\\nQ2: Supervised Text Classification Model: KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3732bf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nQ3: Hyper Parameter Selection - KNN\")\n",
    "\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb6925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yi Jing\n",
    "print(\"Yi Jing\")\n",
    "print(\"\\nQ2: Supervised Text Classification Model: ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bad9130",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nQ3: Hyper Parameter Selection - ?\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
